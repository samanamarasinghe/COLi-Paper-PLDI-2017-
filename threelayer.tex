\section{\label{sec:ir}Three-Layer Intermediate Representation}
In this section, we describe in detail the three representations used in \framework{}: Layer I, which defines the computations; Layer II, which defines a computation schedule; and Layer III, which specifies data layout.

\subsection{Layer I: \Layerone}
\label{layer1}

The first layer defines abstract computations: computations that are not yet scheduled and not yet mapped to memory.  This layer is defined as a set of iteration domains where each iteration domain is composed of a set of computations.  A computation is an expression of other computations and literal constants.  Non-loop statements have an iteration domain of a single point.

Formally, the first layer is a union of \emph{computation sets} such that each computation set describes one statement in the program. Each computation set is defined as follows:
$$\{N1(\vec{s}) | f(\vec{s}, \vec{p})\} : g(N2(\vec{s}), N3(\vec{s}), ..., N4(\vec{s}))$$

\noindent where $N1(\vec{s})$ is a computation that has the name $N1$, $g(N2(\vec{s}), N3(\vec{s}), ..., N4(\vec{s}))$ is the expression that the computation computes and $f(\vec{s}, \vec{p})$ is a Presburger formula that evaluates to true, if and only if $\vec{s}$ is an element of $S$ for the given parameters $\vec{p}$.

Computations in Layer I are in Static Single Assignment (SSA) form~\cite{Cytron:1991:ECS:115372.115320}; each computation is defined once and only once (we use the $\phi$ function to deal with branches as in classical SSA).  To implement algorithms that perform a reduction or that update a variable (i.e. redefine it), we expand the iteration domain and add a new dimension for versioning.  For example, a matrix multiplication would be implemented as follows in the \layerone:
\begin{align*}
\{c(i,j,-1):  0\leq i < N \wedge 0\leq j < N \}: & 0 \\
\{c(i,j,k): 0\leq i < N \wedge 0\leq j < N \wedge 0 \leq k < N \}: & \\
 c(i,j,k-1) + A(i,k) * B(k,j) &
\end{align*}

% When generating the third layer for the matrix multiplication code, each computation $c(i,j,k)$ should be mapped to the buffer \lstinline{bufc[i,j]}.  This will produce a reduction.
When generating Layer III from this code, each computation in the three dimensional domain will be mapped to a 2-dimensional
buffer, producing a reduction.

%$\{c[i,j,k] \rightarrow buf_c[i,j]:  0\leq i < N \wedge 0\leq j < N \wedge 0 \leq k < N \}$

In Layer I, inputs to the program are represented by wrapping them into a buffer.  As a result,  the rest of the program is made memory-independent. For example, in Figure~\ref{fig:example-ir}-\codeone{}, if the input images \lstinline{b1} and \lstinline{b2} were not computed in previous stages of the pipeline but were instead input data, then the Layer I representation would require them to be wrapped into computations:
\begin{align*}
\{w1(i,j):  0\leq i < N \wedge 0\leq j < M \}: &  b1(i,j) \\
\{w2(i,j):  0\leq i < N \wedge 0\leq j < M \}: & b2(i,j) \\
\end{align*}

The appropriate data mapping would be added in Layer III to map computations $w1$ and $w2$ into 
memory accesses into buffers \lstinline{b1} and \lstinline{b2}.
% The following data mapping should have been added in the third layer

% $\{wrapper1(i,j) \rightarrow buf1[i,j]:  0\leq i < N \wedge 0\leq j < N \}$

% $\{wrapper2(i,j) \rightarrow buf2[i,j]:  0\leq i < N \wedge 0\leq j < N \}$

% \noindent This mapping indicates that an access to the computations $wrapper1(i,j)$ and $wrapper2(i,j)$ should be mapped to an access to the buffer elements \lstinline{buff1(i,j)} and \lstinline{buff2(i,j)}.  This wrapping keeps the first layer memory-independent, and thus simplifies the algorithms that operate on that layer, since all the accesses are accesses to computations only.

In the classical polyhedral model, extracting the iteration space requires all loop bounds and conditions to be quasi-affine with respect to the loop iterators and a fixed set of symbolic constants. This condition is called \emph{static-affine}.  In this paper, we use techniques similar to those introduced in~\cite{benabderrahmane_polyhedral_2010,pencil} to support non static-affine iteration spaces.

Computations in the Layer I representation are typed; for brevity we omit types in Figure~\ref{fig:example-ir}.  \framework{} supports primitive datatypes used in C (e.g. \texttt{int8\_t}, \texttt{float}, \texttt{double}, etc.) and tuples, which are equivalent to structures in C.
%  In layer III, we allow the array data type in addition of the previous types.


\subsection{Layer II: \Layertwo}
\label{layer2}

The \layertwo describes when and where each computation is computed.  Unlike computations in the first layer, computations in this layer are ordered (specifying when) and are assigned to a particular processor (specifying where), using a \emph{time-\processor vector}.  Computations in this layer have two types of dimensions: processor dimensions and time dimensions.  Processor dimensions specify on which processor computations should be executed; such dimensions are not relevant for determining the order of execution.  On the other hand,  time dimensions specify the order of execution relative to other computations.  The logical order of execution of computations is determined by their lexicographical order in Layer II.

% To distinguish \processor dimensions from time dimensions, we use tags.  Time dimensions are untagged, while \processor dimensions use tags to indicate the type of processor on which the computation is executed; in addition to the processor type, a tag may indicate the processor is virtual, in which case .  \framework{} supports \emph{cpu}, \emph{gpu}, \emph{node} or \emph{vec} processor types.  The \emph{cpu} tag indicates that the computation will be executed on a CPU thread in a shared memory system, the \emph{gpu} tag indicates an execution on a GPU thread, the \emph{node} tag indicates the node on which the computation is executed in a distributed system while the \emph{vec} indicates the vectorization of the dimension.  

\Processor{} dimensions are distinguished from time dimensions by using tags, which consist of a processor type followed by
zero or more properties.  Currently, \framework{} supports \emph{cpu}, \emph{gpu}, and \emph{node} processor types, for
CPUs on a shared memory system, GPU threads, and nodes in a distributed system, respectively.  Tagging a dimension with
a processor type indicates that the dimension should be distributed over the processors of that type in a system; for example,
tagging a dimension with \emph{cpu} will execute each iteration in that dimension on a separate CPU.

In addition to processor type, tags can optionally include one of two properties: \emph{virtual} and \emph{vec}.  The latter
indicates that the dimension should be vectorized by some constant vector size.  The \emph{virtual} property applies allows
any processor to execute iterations in that dimension, similar to using a work-queue or OpenMP dynamic scheduling.
For example, the time-\processor vector (the left part) in
\begin{align*}
\{b_1[i (cpu), j, 0, k]:  0\leq i < N \wedge 0\leq j < M \wedge 0 \leq k < 2\}: & \\
1.5*f1[i,j] &
\end{align*}

indicates that the computations $b1[i, j, 0, k]$ should be mapped to a thread that runs on the CPU that has $i$ as its ID.  Adding the tag \emph{virtual} indicates that the computation is not mapped to a specific processor but is rather mapped to a thread that can run on any CPU of the shared memory multi-processor system.

Transforming the first layer into the second layer is usually done using an affine relation called the \emph{schedule}. While the first layer is a non-ordered set of computations, the second layer is ordered.  The schedule maps each computation in the first layer into a particular position in the time-\processor space. Composing many transformations can be done simply by composing different schedule relations.  Programmers may specify the schedule directly or use convenience functions such as \lstinline{tile()}.

%T

% non-linear parameters
%In addition to quasi-affine constraints, we allow some cases of non-linear parameters in the layer II set constraints.  In particular, we allow any expression of the loop parameters to be used in the constraints.  For example we allow a constraint such as $i < N/B$ where $N$ and $B$ are both symbolic constants or a constraint such as $i > N + M/N$.  In these cases, the non-linear parameter expression $N/B$ can be replaced by a declaration of a new parameter $p1 = N/B$ at the beginning of the program (or before the loop) and the use of $t1$ in the constraints $i \leq p1$.  For the second example, we represent the constraint as a declaration $p1 = N + M/N$ and a constraint $i > p1$.

More precisely, the second layer is a union of ordered computation sets.
%Each computation set is described as follows:
%$$\{N1[\vec{s}] | f(\vec{s}, \vec{p})\} : g(N2[\vec{s}], N3[\vec{s}], ..., N4[\vec{s}])$$

%\TODO{Fix definition of Layer II, currently it is identical to Layer I}

%\noindent where $N1[\vec{s}]$ is a computation, $g(N2[\vec{s}], N3[\vec{s}], ..., N4[\vec{s}])$ is the expression that the computation computes and $f(\vec{s}, \vec{p})$ is a Presburger formula that evaluates to true, if and only if $\vec{s}$ is an element of $S$ for the given parameters $\vec{p}$.

The order of computations mapped to the same processor is identified by projecting the computation set on the time dimensions and comparing their lexicographical order, without considering the name of the computation since all computations in this layer are in the same time-\processor space; computations in this layer are named only for readability purposes.

% Using a non-affine transformation framework (certain classes of non-affine transformations are possible transformations, a transformation function (a schedule) is defined and used to transform the iteration domain into time-space domain.

%\TODO{Explain an example (of parallelization) (sec. 2)}

\subsection{Layer III: \Layerthree}
\label{layer3}

The \layerthree specifies memory locations where the computed values are stored.  It consists of Layer II computations, additional allocation/deallocation statements,  and a set of \emph{access relations},
which map computations from Layer II to array elements read or written by those computations, where scalars are treated as zero-dimensional arrays.  Buffers used for storage are declared and allocated in this layer. For each buffer, an allocation statement that specifies the type of the buffer (or scalar) and its size is added and scheduled by being mapped to the time-\processor space.  In a similar way, a deallocation statement is also added.

%To create the third layer, we take the second layer and add access relations that indicate where computations should be stored.
More precisely, the third layer is a union of computation sets and a set of access relations.  The computation sets are identical to the Layer II computation sets except that new allocation/deallocation statements are added.  The set of access relations is described as follows:
$$\{N1[\vec{s}] \rightarrow B[\vec{o}], (\vec{s}, \vec{o}) \in \mathbb{Z}^{d_1}\times\mathbb{Z}^{d_2} | f(\vec{s}, \vec{o}, \vec{p})\}$$

\noindent where $N1[\vec{s}]$ is a computation mapped to the buffer element $B[\vec{o}]$ and $f(\vec{s}, \vec{o}, \vec{p})$ is a Presburger formula that evaluates to true if and only if there is a relation from $\vec{s}$ to $\vec{o}$ for the given parameters $\vec{p}$.

Possible data mappings in \framework include mapping computations to structure-of-arrays, array-of-structures, contraction of multidimensional arrays into arrays with less dimensions or into scalars.  It is also possible to specify more complicated accesses such as the storage of the computations $c(i,j)$ into the array elements $c(i\%2,j\%2)$ or into $c(j,i)$.

