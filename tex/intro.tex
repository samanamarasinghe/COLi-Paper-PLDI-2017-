\section{Introduction}
\label{sec:intro}

High performance systems today are in widespread use.  These systems range from multicore mobile phones to large scale heterogeneous super computers and cloud infrastructures equipped with GPUs and FPGAs.  Building a code optimization framework for these high performance systems requires solving many challenges.

\textbf{Challenges.}
The first challenge in is the \emph{diversity of hardware architectures}. It includes the ability to write portable code and the ability to target \emph{distributed and heterogeneous systems} since most high performance computer systems are complex and heterogeneous (they have GPUs~\cite{liao2014milkyway} and FPGAs~\cite{caulfield2017configurable}).
Getting the best performance requires the program to take full advantage of all these different components.  In the supercomputing community this is usually referred to as the MPI+OpenMP+CUDA challenge~\cite{yang2011hybrid}.
Writing code that targets such heterogeneous systems is non-trivial, as each of these require drastically different styles of code and optimization, all using different libraries and languages.  Getting all of these components to communicate and synchronize is non-trivial as well. The state-of-the-art practice is to manually write the program in separate language extensions. However, any changes to the program partitioning between these heterogeneous units will normally require a complete rewrite of the program.  A framework targeting high performance systems needs to address this challenge.

The second challenge is related to the program \emph{optimization}.  In many performance critical domains, the priority of users is to have the fastest code.  Generating code that is comparable in performance to highly optimized codes is important.  
Generating such code requires a series of non-trivial program transformations that optimization frameworks try to fully automate using cost models, heuristics~\cite{hall1995detecting}, and machine learning~\cite{tournavitis2009towards}.
While these automatic optimization frameworks provide a good level of productivity, they may not always achieve the desired level of high performance and this limits their usability in performance critical domains.
%A code optimization framework targeting high performance systems should be able to generate code comparable in performance to highly-optimized hand-written code.

The third challenge is that of \emph{memory dependence}.  Most intermediate representations use memory as a means of communication between program statements.  This creates memory-based dependencies in the program.  It also means that the data-layout is chosen before deciding how the code should be optimized and mapped to hardware.  Optimizing a program for different hardware architectures usually requires modifying the data-layout and requires eliminating memory-based dependencies since they restrict optimization~\cite{maydan1992data}.  Thus, any data-layout specified before scheduling has to be undone to allow more freedom for scheduling and then it has to be transformed into another data-layout that takes full advantage of the target hardware.
Applying such data-layout transformations and the elimination of memory-based dependencies is, in general, challenging~\cite{gupta1997privatization,autoPrivatPeng,li_array_1992,feautrier_array_1988,midkiff_automatic_2012,maydan_array-data_1993,lefebvre_automatic_1998,Qui00,Darte_contraction_2005}.  An optimization framework aiming to succeed in targeting diverse hardware architectures needs to solve this challenge.

The forth challenge is that of \emph{representation}.  Lowering code to complex hardware architectures requires many complex transformations that change the loop structure in complex ways by introducing new loops, complex loop bounds, and non-trivial array accesses~\cite{wolf1991loop}.  Analyzing code generated by one of these transformations is challenging, which complicates composition with other transformations.  This problem can be solved if the loops are kept within a single unified representation through all the transformations.  However, many representations are inadequate or too conservative to support complex transformations.  They are also inadequate for performing tasks such as dependence analysis (which is necessary for deciding about the correctness of optimization) and tasks such as the computation of communication sets (data to send and receive in a distributed system).  Having a representation that is expressive enough is important for the success of an optimization framework targeting high performance systems.

\textbf{\framework{}.}
This paper addresses the previous challenges by introducing \framework{}, a compiler optimization framework designed for targeting high performance systems.  \framework{} takes a high level representation of the program (pure algorithm and a set of commands specifying the schedule and data layout), optimizes that representation and generates code highly optimized for the target architectures.  \framework{} is designed to hide the complexity and the large variety of execution platforms by providing a multi-layer representation suitable for transforming from high-level languages to multicore CPUs, GPUs, distributed machines, and FPGAs.

\framework{} addresses the difficulty of programming distributed heterogeneous systems by allowing users to specify communication and to map the application into heterogeneous hardware from the same source code using a simple set of commands.  \framework{} solves the challenge of hardware diversity and the challenge of memory dependence by using a novel multi-layer IR that fully separates the architecture independent algorithm from its schedule and data layout.
The multi-layer design exposes four layers that make it easier to perform each program transformation at the right layer of abstraction.
The top layer representation describes the pure algorithm using producer-consumer relationships without memory locations.
The second layer specifies the order of the computations, along with which processor computes each value; this layer is suitable for performing a vast number of optimization without dealing with concrete memory layouts.
The third layer specifies where to store intermediate data before they are consumed.  It also adds any communication and synchronization operations.  The fourth layer schedules the newly added communication and synchronization operations (provides their order and on which processor they execute).
\framework{} then lowers the fourth layer to a target architecture while performing back-end-specific code optimization.
%This design fully separates the architecture-independent front-end from the architecture-specific backend.

The challenge of optimization is addressed in \framework{} by relying on two principles: the first principle is the separation of mechanism from policy in scheduling.  
\framework{} provides mechanism for scheduling using a set of primitives that can transform and map the programs to the underlying hardware.
It does not provide a policy to automatically decide which optimization to apply.  Languages such as Halide~\cite{halide_12,chill} have shown that this approach is a viable one.  \framework{} generalizes that approach to the distributed and heterogeneous domain and provides a broader set of transformations and a seamless way to compose them.
The second principle is that languages should not prevent the user from applying a legal schedule.  Certain languages such as Halide, for example, take a conservative approach to guarantee the correctness of optimization by allowing only optimization that are always legal.  Optimization that may be illegal in certain cases are not allowed.  This is too restrictive as it prevents transformations such as the fusion of two loops one updating an array and the other reading that array.  \framework{} in contrast is more permissive because it uses dependence analysis to guarantee the correctness of optimization.

\framework{} addresses the challenge of representation by using a unified framework based on polyhedral sets to represent the four layers.  This makes it simple for \framework{} to reason about and implement iteration space and data layout transformations, since these are represented as transformations on polyhedral sets.  It also simplifies deciding about the legality of transformations (based on dependence analysis).  The use of a unified general framework relieves compiler developers from developing custom algorithms and analyses that only work in certain situations and cannot be composed with other optimization passes.\newline

\noindent \textbf{Contributions of the paper}.
\vspace{-0.2cm}
\begin{itemize}
  \item We show that a unified framework can generate code for multiple high-performance architectures such as multicore CPUs, GPUs, FPGAs, distributed machines, or any combination of these, using a set of simple scheduling commands to guide the program transformation.

  \item We introduce a framework designed to generate fast code (the first to match the performance of Intel MKL gemm).

  \item We introduce a novel four-layer IR that separates the algorithm from the code transformations and the data layout transformations, simplifying the composition of architecture-specific lowering transformations.

  \item We implemented this framework, \framework{}, and demonstrate its power and viability by using it to write multiple linear algebra and neural network kernels and by using it as an optimization framework for  Halide~\cite{halide_12,DBLP:conf/pldi/Ragan-KelleyBAPDA13}.

  \item We demonstrate the expressiveness of \framework{} by extending Halide with many new capabilities such as expressing code with cyclic flow graph, performing precise bound inference for non-rectangular iteration spaces, and performing advanced loop nest transformations such as skewing, shifting, and loop fusion.

  \item We evaluate \framework{} and show that we match the performance of highly optimized libraries such as Intel MKL and show up to $4\times$ performance gains in Halide.
\end{itemize}
