
\section{Introduction}
\label{sec:intro}

%Traditional compilers are layered into a set of front-end architecture-independent passes and back-end architecture-dependent passes.
With the recent increase of diversity in hardware platforms, the development of DSL (Domain Specific Language) compilers is proliferating~\cite{halide_12,DBLP:conf/pldi/Ragan-KelleyBAPDA13,diedrot_pldi12,DBLP:journals/toms/AlnaesLORW14,polymage,bezanson2017julia,tensorflow}.
Many of these DSL compilers target intermediate representations (IRs) similar to the LLVM IR~\cite{llvm}.  Such IRs commit to a specific data layout early before deciding about how the program should be optimized or on which hardware it should be mapped.  This early binding reduces optimization opportunities, adds unnecessary dependences such as anti- and output- dependences and forces the compiler to modify the data layout in order to be able to perform certain optimizations.  For example, compilers privatize~\cite{gupta1997privatization,autoPrivatPeng,li_array_1992} and expand~\cite{feautrier_array_1988,midkiff_automatic_2012,maydan_array-data_1993} arrays and scalars in order to be able to parallelize loop nests, the arrays are then contracted~\cite{lefebvre_automatic_1998,Qui00,Darte_contraction_2005} to minimize memory footprint.  Many other techniques such a variable renaming and false dependence elimination~\cite{Baghdadi:fdeps,Mehta:2016:VL:2988523.2963101} were designed to undo/modify the memory data layout in order to enable transformations that are not possible otherwise.  Since DSLs usually do not represent the memory layout, a DSL IR would more naturally be organized in at least two layers.  In the first layer, the data layout is not specified which means that the compiler can perform optimizations that should not be constrained by the data layout. In the other layer, the data-layout is specified which means that the compiler can perform any optimization that does not need to modify the data layout.  This design eliminates the need to undo/modify the data layout.

In a similar way, many algorithmic level optimizations such as operator inlining (which substitutes a producer in a consumer) and operator fusion (which fuses two operators into one operator) are better performed on the original unoptimized algorithm since optimizations such as skewing, vectorization and mapping to a distributed memory system obfuscate the code.
This suggests that certain optimizations should better be performed on the original algorithm before optimization.  This also suggests that the IR should be further divided into a layer where only computations are described (the pure algorithm) and another layer where computations are scheduled (optimized). Optimizations that are affected by code transformations should be applied at the first level.

In this paper, we introduce \framework{}, a middle-end compiler designed around the idea of multi-layer IRs.  The top layer, the {\it \Layerone}, describes the pure algorithm (the computations to perform). At this level, memory locations are not considered and all dependences are represented using producer-consumer relationships.  The second layer, the {\it \Layertwo}, specifies when and on which processor each computation is performed (time and space).  Any optimization that should not be constrained by a data layout should be performed at this layer.  The final layer, {\it \Layerthree}, specifies where to store produced data until they are consumed.
%Within \framework{}, each layer can be automatically lowered to the subsequent layer; however, if this mapping needs to be controlled by the DSL compiler, \framework{} exposes the representations using a unified representation.

Before generating a low level such as the LLVM IR, DSL compilers need to perform transformations to take advantage of most modern hardware features such as multicore parallelism, complex non-uniform memory (NUMA) hierarchies, clusters, and accelerators like graphics processing units (GPUs). As a result, the DSL compilers must expand into {\it middle-end} compilers performing all modern architectural optimizations.  While the diversity of front-end compilers is necessary, the middle-end compilers mostly re-implement the same set of optimizations for the same set of modern architectural features.  \framework is designed to eliminate this redundancy, it is a common middle-end compiler framework that removes the burden of building middle-end compiler passes into every DSL compiler.

\framework{} uses a unified set-based framework to perform many tasks that a typical middle-end compiler needs to perform.
Examples of such tasks include the ability to perform common iteration space and data layout transformations, the ability to compose these transformations and to perform dependence analysis in order to decide about the legality of transformations, ...  
\framework{} represents the three layers using polyhedral sets.  All of the previous operations, including iteration space and data layout transformations, can be represented using basic operations on these sets.  We use the ISL (Integer Set Library)~\cite{verdoolaege_isl:_2010} to implement these sets and set operations.
This design is in contrast to the use of multiple representations, algorithms and techniques to perform all of previous tasks which would increase the complexity of the compiler.

%One argument against having a multi-layer IR is that usually in a compiler certain passes need to be called multiple times during the process of code lowering, for example data dependence analysis needs to be performed before transforming a code (to check whether applying the transformation is legal) and after transforming the code (to check if the transformation generated valid code).  This is not a problem in \framework{} for two reasons: first, because we use a unified set-based framework to implement the different layers; in most cases, the same pass that applies to one layer applies to the other layers too. For example, the core of our dependence analysis is exactly the same for the three layers; second, because in a middle-end compiler, most passes are usually high level, specialized passes that do not need to be repeated multiple times unlike in a low level IR like the LLVM IR where simplification and dead code elimination passes need to be called frequently.

\framework{} is not an automatic parallelizing compiler; transformation decisions are left to the front-end compiler, with \framework{} providing mechanisms to implement these decisions without unnecessary complexity.
% \framework{} provides three methods with increasing complexity that the front-end can use to control middle-end transformations. The first method is a simple set of transformation commands, similar to Halide~\cite{halide_12} or ChiLL~\cite{chill}. The second is the ability to express the \Layertwo directly to indicate when and where computations should be executed.
% The third is an {\it affine schedule}. 
All the difficult work of bookkeeping, code and data transformations, and generating correct code to utilize specific hardware features is done by \framework{}, eliminating the need to build a full middle-end compiler per DSL.

This paper makes the following contributions:
\begin{itemize}
  \item We introduce a middle-end compiler build around the idea of a \emph{multi-layer} IR. This IR has a three layers.  The  three layers enable the separation between the algorithm, the schedule and the data layout. The first layer describes computations without describing how these computations are scheduled or where the intermediate values are stored.  The second layer describes how computations are scheduled but does not represent where the values are stored.  Performing optimizations that should not be constrained by the data layout at this level becomes easier, since code transformation passes do not need to transform the data layout. The third layer specifies data layout.
  \item We use a unified, set-based representation to represent the three layers and to perform many tasks needed by the \framework{} middle-end compiler.  The use of one framework simplifies greatly the development of \framework{}.
  \item An open source implementation of \framework{} that demonstrates its power and viability:
  \begin{itemize}
    \item \framework{} is used as a middle-end for two DSL compilers: Halide~\cite{halide_12,DBLP:conf/pldi/Ragan-KelleyBAPDA13} and Julia~\cite{bezanson2017julia};
    \item \framework{} implements more than 38 high level  commands for iteration space and data layout transformations.
    \item \framework{} targets four hardware architectures: multicore CPUs with vectorization, GPUs, FPGAs and distributed systems.
  \end{itemize}
  \item We demonstrate the expressiveness of \framework{} by  extending Halide with many new capabilities: the ability to express and tile recurrent filters, the ability to express rectangular iteration spaces and perform precise bound inference in such case, the ability to perform dependence analysis and decide about the correctness of optimizations, the ability to perform advanced loop nest transformations such as skewing, shifting and loop fusion in the general case, and the ability to map loop nests to distributed memory systems.
  \item We evaluate \framework{} and show up to 4$\times$ performance gains in Halide and 5$\times$ in Julia.
\end{itemize}
