\vspace{-0.25cm}
\section{The \framework Embedded DSL}

\framework{} is a domain-specific language (DSL) embedded in C++. It provides a pure C++ API that allows users to write a high level, architecture-independent algorithm and a set of scheduling and data mapping commands that guide code generation.
The input \framework code can either be written directly by a programmer, or generated by a different DSL compiler.  \framework{} then constructs a high level intermediate representation (IR), applies the user-specified code and data-layout transformations, and generates optimized backend code (LLVM IR and CUDA) that takes advantage of the target hardware features.


\vspace{-0.25cm}
\paragraph{\textbf{Scope}}
\framework is designed for expressing data parallel algorithms, especially those that operate over dense arrays using loop nests and sequences of statements.  These algorithms are often found in the areas of dense linear algebra and tensor algebra, stencil computations, image processing, and deep neural networks.

\vspace{-0.3cm}
\subsection{Specifying the Algorithm}
The first part of a \framework{} program specifies the algorithm without specifying the schedule (when and where the computations occur), how data should be stored in memory (data-layout), or communication.
At this level there is no notion of data location; rather, values are communicated via explicit producer-consumer relationships.

The algorithm is a pure function that has inputs, outputs, and a sequence of statements.  The statements are called \textit{computations} in \framework.  Flow-control around these computations is restricted to \texttt{for} loops and conditionals;  \texttt{while} loops, early exits and \texttt{GOTO}s cannot be expressed.  To declare a computation, the user provides both the iteration domain of the computation and the expression to compute.  %In the rest of this section, we will provide an example of the API used to declare computations in \framework.

Figure~\ref{fig:algorithm} shows a blur algorithm written in \framework, using \texttt{bx} and \texttt{by}, which are the two computations in this algorithm.
The first computation, \texttt{bx}, computes a horizontal blur of the input, while the second computation, \texttt{by}, computes the final blur \texttt{by} averaging the output of the first stage.
The iterators \texttt{i}, \texttt{j} and \texttt{c} in line~\ref{fig:example:tiramisu:iterators} define the iteration domain of \texttt{bx} and \texttt{by} (for brevity we ignore boundary conditions).
The algorithm is semantically equivalent to the following code.
%By default, all of the computed values are stored in memory. Each computation has an implicit buffer associated with it where the number of dimensions of this buffer is equal to the number of dimensions in the iteration domain.

\begin{figure}
\begin{lstlisting}[language=C,escapechar=@]
// Declare the iterators i, j and c.
var i(0, N-2), j(0, M-2), c(0, 3);@\label{fig:example:tiramisu:iterators}@

// Algorithm.
bx(i,j,c) = (in(i,j,c)+in(i,j+1,c)+in(i,j+2,c))/3;@\label{fig:example:tiramisu:computation1}@
by(i,j,c) = (bx(i,j,c)+bx(i+1,j,c)+bx(i+2,j,c))/3@\label{fig:example:tiramisu:computation2}@);
\end{lstlisting}
\vspace{-0.25cm}
\caption{\label{fig:algorithm}\texttt{Blur} algorithm without scheduling commands.}
\vspace{-0.25cm}
\end{figure}

\begin{lstlisting}[language=C,escapechar=@]
for (i in 0..N-2)
 for (j in 0..M-2)
  for (c in 0..3)
   bx[i][j][c] = (in[i][j][c]+in[i][j+1][c]+in[i][j+2][c])/3
for (i in 0..N-2)
 for (j in 0..M-2)
  for (c in 0..3)
   by[i][j][c] = (bx[i][j][c]+bx[i+1][j][c]+bx[i+2][j][c])/3
\end{lstlisting}


\vspace{-0.25cm}
\subsection{Scheduling Commands}

\framework provides a set of high-level scheduling commands for common optimizations. Table~\ref{tab:scheduling} shows examples of these commands.  There are four types of scheduling commands:
\begin{itemize}
    \item Commands for loop nest transformations: they include common affine transformations such as loop tiling, splitting, shifting, etc.  For example, applying a 32$\times$32 loop tiling on a computation \texttt{C} can be done by calling\\ \texttt{C.tile(i,j,32,32,i0,j0,i1,j1)} where \texttt{i} and \texttt{j} are the original loop iterators and \texttt{i0}, \texttt{j0}, \texttt{i1}, and \texttt{j1} are the names of the loop iterators after tiling.

    \item Commands for mapping loop levels to hardware: these include loop parallelization, vectorization, mapping loop levels to a GPU block or thread dimension. For example calling \texttt{C.vectorize(j, 4)} splits the \texttt{j} loop by a factor of 4 and then maps the inner loop to vector  instructions.

    \item Commands for manipulating data: these include (1) allocating arrays; (2) setting array properties including whether the array is stored in host, device, shared, or local memory (GPU); (3) copying data (between levels of memory hierarchies or between two nodes);  (4) setting array accesses. In most cases, users need only use high level commands for data manipulation. If the high level commands are not expressive enough, the user can use the more expressive low level commands.

    \item Commands for adding synchronization operations: the user can either declare a barrier or use the \texttt{send} and \texttt{receive} functions for point-to-point synchronization.
\end{itemize}

\begin{table}[t]
    \scriptsize
    \setlength\tabcolsep{1pt}
    \begin{tabular}{l|l}
        \hline
        \multicolumn{2}{c}{\textbf{We assume that C and P are computations, b is a buffer, i and j are loop iterators}} \\\hline
        \multicolumn{2}{c}{\textbf{Commands for loop nest transformations}} \\\hline
        \textbf{Command} & \textbf{Description} \\\hline
        \begin{tabular}{ll} \texttt{C.tile(}& \texttt{i,j,t1,t2,}\\ & \texttt{i0,j0,i1,j1)}
        \end{tabular} & 
        \begin{tabular}{l}
        Tile the dimensions (i,j) of the computation C by $t1\times t2$.\\
        The names of the new dimensions are (i0, j0, i1, j1), where \\
        (i0, j0) are the outer tiles and (i1, j1) are the inner tiles.\end{tabular}\\ \hline
        \texttt{C.interchange(i, j)} & Interchange the dimensions of C (loop interchange) \\\hline
        \texttt{C.shift(i, s)} & Loop shifting (shift the dimension i by s iterations) \\ \hline
        \texttt{C.split(i, s, i0, i1)} & Split the dimension i by s. (i0, i1) are the new dimensions\\ \hline
        \texttt{P.compute}\_at(C, j) & Compute the computation \emph{P} in the loop nest of \emph{C} at loop\\
        & level j.  This might introduce redundant computations.\\
        \hline
        \texttt{C.unroll(i, v)} & Unroll the dimension i by a factor v\\\hline
        \texttt{C.after(B, i)} & Indicate that C should be ordered after B at the loop level i\\
        &(they have the same order in all the loop levels above i)\\
        \hline
        \texttt{C.inline()} & Inline C in all of its consumers\\ \hline
        \texttt{C.set\_schedule()} & Set an affine transformation for C (to transform Layer I to II)\\\hline
        \multicolumn{2}{c}{\textbf{Commands for mapping loop levels to hardware}} \\\hline
        \texttt{C.parallelize(i)} & Mark the dimension i as a space dimension (cpu)\\\hline
        \texttt{C.vectorize(i, v)} & Vectorize the dimension i by a vector size v\\\hline
        \texttt{C.gpu(i0, i1, i2)} & Mark the dimensions i0, i1 and i2 to be executed on the GPU \\\hline
        \texttt{C.tile\_gpu(i0, i1)} & Tile the loops i0 and i1 and map them to GPU \\\hline
        \texttt{C.distribute(i)} & Mark the dimension i as a space dimension (node)\\\hline
        \multicolumn{2}{c}{\textbf{High level commands for data manipulation}} \\\hline
        \texttt{\textbf{C.store\_in(b,\{i, j\})}} & Store the result of the computation C(i,j) in b[i,j].\\\hline
        \texttt{\textbf{C.cache\_shared\_at(C, i)}} & Cache (copy) the buffer of C in shared memory. The amount\\
        & of data to copy, the access functions, and synchronization are\\
        & computed automatically.\\\hline
        \texttt{\textbf{C.cache\_local\_at(C, i)}} & Similar to \texttt{cache\_shared\_at} but stores in local GPU memory.\\\hline
        \texttt{\textbf{send(d, src, s, q, p)}}
            & Create a send operation. \texttt{d}: vector of iterators to represent \\
            &  the iteration domain; \texttt{src}: source buffer; \texttt{s}: size; \texttt{q}: destination \\
            & node; \texttt{p}: properties (synchronous/asynchronous, blocking, ...).
        \\\hline
        \texttt{\textbf{receive(d, dst, s, q, p)}}
            & Create a receive operation. Arguments similar to \texttt{send} except\\
            &\texttt{q} which is the source node.
        \\\hline
        \multicolumn{2}{c}{\textbf{Low level commands for data manipulation}}\\\hline
        \texttt{\textbf{Buffer b(sizes, type)}} & Declare a buffer (\texttt{s}: a vector of dimension sizes, \texttt{t}: type). \\\hline
        \texttt{\textbf{b.allocate\_at(p, i)}} & Return an operation that allocate b at the loop i of p.\\\hline
        \texttt{\textbf{C.buffer()}} & Return the buffer associated to the computation C.\\\hline
        \texttt{\textbf{b.set\_size(sizes)}} & Set the size of a buffer. \texttt{sizes}: a vector of dimension sizes.\\\hline
        \texttt{\textbf{b.tag\_gpu\_global()}} & Tag this buffer to be stored in global GPU memory.\\\hline
        \texttt{\textbf{b.tag\_gpu\_shared()}} & Tag this buffer to be stored in shared GPU memory.\\\hline
        \texttt{\textbf{b.tag\_gpu\_local()}} & Tag this buffer to be stored in local GPU memory.\\\hline
        \texttt{\textbf{b.tag\_gpu\_constant()}} & Tag this buffer to be stored in constant GPU memory.\\\hline
        \texttt{\textbf{C.host\_to\_device()}} & Return an operation that copies C.buffer() from host to device.\\\hline
        \texttt{\textbf{C.device\_to\_host()}} & Return an operation that copies C.buffer() from device to host.\\\hline
        \texttt{\textbf{copy\_at(p, i, bs, bd)}}
            & Return an operation that copies the buffer \texttt{bs} to the buffer \texttt{bd} at\\
            & the loop i of p.
            Used for copies between global, shared and local.
        \\\hline
        \multicolumn{2}{c}{\textbf{Commands for synchronization}} \\\hline
        \texttt{\textbf{barrier\_at(p, i)}}
            & Create a barrier at the loop p of i.
        \\\hline
    \end{tabular}
    \caption{Examples of \framework{} Scheduling Commands}
    \label{tab:scheduling}
    \vspace{-0.75cm}
\end{table}

The novel commands that \framework introduces are highlighted in \textbf{bold} in Table~\ref{tab:scheduling}.  They include array allocation, copying data between memory hierarchies, sending and receiving data between nodes, and synchronization.  Calls to \texttt{cache\_shared\_at()}, \texttt{cache\_local\_at()}, \texttt{allocate\_at()}, \texttt{copy\_at()}, \texttt{barrier\_at()} return an operation that can be scheduled like any other computation.
The operations \\ \texttt{cache\_shared\_at()} and \texttt{cache\_local\_at()} can be used to create a cache for a buffer (GPU only).  They automatically compute the amount of data that needs to be cached, perform the data copy, and insert any necessary synchronization.

\begin{figure*}[t]
    \centering
    \scriptsize
    \setlength\tabcolsep{4pt}
\begin{tabular}{cl|@{}l}
 &
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
    \textbf{\framework Scheduling Commands}
 &
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
    \textbf{Pseudocode Representing Code Generated by \framework}
\\\hline

{\textbf{\normalsize(a)}} &

\begin{lstlisting}[language=C,escapechar=@]
// Scheduling commands for targeting GPU.
// Tile i and j and map the resulting dimensions to GPU
var i0, j0, i1, j1;
by.tile_gpu(i, j, 32, 32, i0, j0, i1, j1);
bx.compute_at(by, j0);
bx.cache_shared_at(by, j0);

// Use struct-of-array data layout for bx and by.
bx.store_in({c,i,j}); by.store_in({c,i,j});

// Create data copy operations
operation cp1 = in.host_to_device();
operation cp2 = by.device_to_host();

// Specify the order of execution of copies
cp1.before(bx, root); cp2.after(by, root);
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

& 

\begin{lstlisting}[language=C,escapechar=@]
 host_to_device_copy(in_host, in);

 @{\color{listingkeywordcolor}{\textbf{GPUBlock}}}@ for(i0 in 0..floor(N-2,32))
  @{\color{listingkeywordcolor}{\textbf{GPUBlock}}}@ for(j0 in 0..floor(M-2,32))
   @{\color{listingkeywordcolor}{\textbf{shared}}}@ bx[3,32,34];
   @{\color{listingkeywordcolor}{\textbf{GPUThread}}}@ for(i1 in 0..min(N-2,32*i0+31+2))
    @{\color{listingkeywordcolor}{\textbf{GPUThread}}}@ for(j1 in 0..min(M-2,32*j0+31+2))
     for (c in 0..3)
      bx[c][i1-32*i0][j1-32*j0]=
        (in[i1][j1][c]+in[i1][j1+1][c]+in[i1][j1+2][c])/3@\label{fig:motivating:code2:stmt2}@
   @{\color{listingkeywordcolor}{\textbf{GPUThread}}}@ for(i1 in 0..min(N-2,32*i0+31))
    @{\color{listingkeywordcolor}{\textbf{GPUThread}}}@ for(j1 in 0..min(M-2,32*j0+31))
     for (c in 0..3)
      by[c][i1][j1]=(bx[c][i1][j1]+bx[c][i1+1][j1]+bx[c][i1+2][j1])/3@\label{fig:motivating:code2:stmt3}@

 device_to_host_copy(by, by_host);

\end{lstlisting}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\\\hline
{\textbf{\normalsize(b)}} &
\begin{lstlisting}[language=C,escapechar=@]
// Scheduling commands for targeting a distributed system
// Declare additional iterators
var ii(0,2), jj(0,M), qs(1,RANKS), qr(0,RANKS-1), q, z;

// Split loop i into loops q and z and parallelize z
bx.split(i,N/RANKS,q,z); bx.parallelize(z);@\label{line:split1}@
by.split(i,N/RANKS,q,z); by.parallelize(z);@\label{line:split2}@ 

// Create communication and order execution
send s = send({qs}, bx(0,0,0), 2xMx3, qs-1, {ASYNC});@\label{line:send}@
recv r = receive({qr}, bx(N,0,0), 2xMx3, qr+1, 
                 {SYNC}, s);@\label{line:recv}@
s.before(r,root); r.before(bx,root)

// Distribute the outermost loops
bx.distribute(q); by.distribute(q); @\label{line:comp_dist}@
s.distribute(qs); r.distribute(qr); @\label{line:comm_dist}@
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% // qs-1 is the rank of the receiving node
%// qr+1 is the rank of the sending node

& 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lstlisting}[language=C,escapechar=@]

 @{\color{listingkeywordcolor}{\textbf{distributed}}}@ for (qs in 1..RANKS) @\label{line:distfor}@
   send(bx(0,0,0), 2xMx3, qs-1,{ASYNC,BLK})
 @{\color{listingkeywordcolor}{\textbf{distributed}}}@ for (qr in 0..RANKS-1)
   recv(bx(N,0,0), 2xMx3, qr+1, {SYNC,BLK})

 @{\color{listingkeywordcolor}{\textbf{distributed}}}@ for (q in 0..RANKS)
   @{\color{listingkeywordcolor}{\textbf{parallel}}}@ for (i in 0..N/RANKS)
    for (j in 0..M)
      for (c in 0..3)
        bx[i][j][c] = (in[i][j][c]+in[i][j+1][c]+in[i][j+2][c])/3@\label{fig:motivating:code2:stmt1}@
 @{\color{listingkeywordcolor}{\textbf{distributed}}}@ for (q in 0..RANKS)
   @{\color{listingkeywordcolor}{\textbf{parallel}}}@ for (i in 0..N/RANKS)
    for (j in 0..M)
      for (c in 0..3)
        by[i][j][c] = (bx[i][j][c]+bx[i+1][j][c]+bx[i+2][j][c])/3
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\\\hline
\end{tabular}
\vspace{-0.25cm}
\caption{Two examples illustrating \framework{} scheduling commands (left) and the corresponding generated code (right). (a) presents a set of scheduling commands for mapping to GPU; (b) presents a set of scheduling commands for mapping to a distributed CPU machine}
\label{fig:mainexample}
\vspace{-0.25cm}
\end{figure*}


The use of \texttt{allocate\_at()}, \texttt{copy\_at()}, and \texttt{barrier\_at()} allows \framework to automatically compute iteration domains for the data copy, allocation, and synchronization operations.  This is important because it relieves the user from guessing and computing the iteration domain manually, especially when exploring different possible schedules.  To illustrate this, consider the example of copying a buffer from global memory to shared memory in a loop nest executing on a GPU.  The size of the area to copy and the iteration domain of the copy operation itself (which is a simple assignment in this case) depends on whether the loop is tiled, the tile size, and  whether any other loop transformation has already been applied. Computing the size of the area to copy in this case is not trivial.  \framework simplifies this step by computing the iteration domain and the area of data to be copied from the schedule directly.

To illustrate more \framework{} scheduling commands, let us take the \texttt{blur} example again from Figure~\ref{fig:algorithm},
and map the two outermost loops of \texttt{bx} and \texttt{by} to GPU.  The necessary scheduling commands are shown in Figure~\ref{fig:mainexample}-\codetwo{} (left).
The \texttt{tile\_gpu()} command tiles the computations then maps the new loops to GPU block and thread dimensions.  The \texttt{compute\_at()} command computes the tiles of \texttt{bx} that need to be read by \texttt{by}.  This transformation introduces redundant computations (in this case) and is known as overlapped tiling~\cite{Krishnamoorthy:2007:EAP:1273442.1250761}.
\texttt{cache\_shared\_at()} instructs \framework to store the results of the \texttt{bx} computation in shared memory.
The subsequent scheduling command (\texttt{store\_in()}) specifies the access functions of \texttt{bx} and \texttt{by}.  In this case, it indicates that these computations are stored in a SOA (struct-of-array) data layout (to allow for coalesced accesses).  The final commands create data copy operations (host-to-device and device-to-host) and schedule them.

Suppose we want to distribute the \texttt{blur} example and run it on a distributed system with multicore CPU nodes. Figure \ref{fig:mainexample}-\codethree{} (left) shows the scheduling commands to use in this case. The \texttt{split()} command splits the outer loop, \texttt{i}, of \texttt{bx} by a splitting factor of \texttt{N/RANKS}.  It creates two new loops \texttt{q} and \texttt{z} such that the outer loop, \texttt{q}, iterates over the number of \texttt{RANKS} (i.e. the number of processes we want to distribute).  The \texttt{q} loop iterator will be mapped later to an MPI \texttt{rank}.  We also parallelize the new inner loop, \texttt{z}, and perform the same transformations on \texttt{by}.

\texttt{create\_send()} and \texttt{create\_recv()} define communication, which in this case, sends $2\times M \times3$ contiguous data elements between nodes starting from \texttt{bx(0,0,0)}. The receiving node consumes the sent data starting from \texttt{bx(N,0,0)}. \texttt{qs} and \texttt{qr} represent the iteration domains. \texttt{qs-1} and \texttt{qr+1} represent the send's destination node and the receive's source node, respectively. \texttt{\{ASYNC\}} defines an asynchronous send and \texttt{\{SYNC\}} defines a synchronous receive.
In this example, we explicitly specify the send and receive operations for illustrative reasons, instead of relying on \framework to automatically insert them.
Explicit send and receive operations are only useful if the array accesses or the iteration domain are not affine (more details about non-affine code in Sec.\ref{nonaffine}) because in such cases automatically computing the amount of data to exchange may not be accurate (the experimental section shows such cases).
Finally, we tag the appropriate loops (the outer loops of \texttt{bx}, \texttt{by}, \texttt{s}, and \texttt{r}), to be distributed (i.e., we tag each iteration to be run on a different node).

%Internally, these distributed \texttt{for} loops (shown in \ref{fig:mainexample}-\codethree{} on the right) will become \texttt{if} statements that check the rank of the currently executing process to see if it is within the range of allowed ranks. For example, the \texttt{distributed for} loop on line \ref{line:distfor} would internally become \texttt{qs=get\_rank();} \texttt{if} \texttt{(qs>=1} \texttt{and} \texttt{qs<RANKS)} \texttt{then... }
% A pseudocode of the code that we which to generate is shown in the right side of the same figure.
% For this example, we want to distribute the computation such that each MPI \emph{rank} (process) operates on contiguous rows of input data. Each rank gets \lstinline{chunk_sz} rows. On line \ref{line:split}, the outer loop is split by \lstinline{chunk_sz}. The resulting inner loop ranges over the rows in the chunk, and the outer loop ranges over the number of MPI ranks we want to use.

% Line \ref{line:send} and \ref{line:recv} deal with communication. We assume that our image data is already distributed, thus only boundary rows need to be communicated among adjacent ranks. We use two-sided communication in \framework{}, meaning communication is done with pairs of \emph{send} and \emph{receive} statements. Line \ref{line:send} defines an asynchronous blocking send operation to processor q-1.  \lstinline{create_send} takes as input the send iteration domain (which also defines the size of data to send), destination rank, communication type, and access into the producer.  Line \ref{line:recv} defines the receive operation.

% Line \ref{line:comp_dist} tags dimension \lstinline{y1} of \lstinline{bx} and \lstinline{by} as distributed, and line \ref{line:comm_dist} tags dimension \lstinline{q} of the \lstinline{send} and \lstinline{receive} as distributed.

All the other scheduling commands in \framework{} can be composed with transfers and distributed loops, as long as the composition is semantically correct. 
%This means we can do everything from basic transformations such as tiling a transfer to more advanced transformations including specializing a distributed computation based on rank.