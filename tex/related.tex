\vspace{-0.25cm}
\section{Related Work\label{related}}

\paragraph{Polyhedral compilers with automatic scheduling.}

Polyhedral compilers such as PENCIL~\cite{pencil,pencil_paper}, Pluto~\cite{bondhugula_practical_2008}, Polly~\cite{polly}, Tensor Comprehensions~\cite{Vasilache2018TensorCF}, and PolyMage~\cite{Mullapudi:2015:PAO:2786763.2694364} are fully automatic.  Some of them are designed for specific domains (such as Tensor Comprehensions and PolyMage), while Pluto, PENCIL, and Polly are more general.
While such fully automatic compilers provide productivity, they may not always provide the best performance.  This is due to many reasons: first, these compilers do not implement some key optimizations such as array packing~\cite{Goto:2008:AHM:1356052.1356053}, register blocking, data prefetching, and asynchronous communication (which are all supported by \framework{}); second, they do not have a precise cost-model to decide which optimizations are profitable in which context.  For example, the Pluto~\cite{bondhugula_practical_2008} automatic scheduling algorithm (which is used for automatic scheduling in Pluto, PENCIL, Polly, and Tensor Comprehensions) tries to minimize the distance between producer and consumer statements while maximizing outermost parallelism, but it does not consider the data layout, redundant computations, the complexity of the control of the generated code, etc.  The PolyMage compiler uses a custom scheduling algorithm designed for the field of image processing, but such an algorithm does not address all the complexities that arise when targeting GPUs and distributed systems.
Instead of fully automatic scheduling, \framework uses a more pragmatic approach and relies on a set of scheduling commands, giving the user full control over scheduling.

% and to map buffers to the right memory hierarchy level; it needs to decide about the correctness of non-affine and data-layout transformations; it needs ; and it needs a sophisticated search technique to search efficiently the very large space of optimizations.  

Polyhedral frameworks proposed by ~\citet{Amarasinghe:1993:COC:173262.155102} and~\citet{6877466} address the problem of code generation for distributed systems. \framework{} makes a different design choice, relying on the user to provide scheduling commands to control choices in the generated code (synchronous/asynchronous communication, the granularity of communication, buffer sizes, when to send and when to receive, explore communication versus re-computation, etc.).

Even though \framework{} focuses on providing a mechanism for code optimization (i.e., it provides scheduling commands for controlling how the program should be optimized), it is still possible to build a framework that provides policy on top of \framework{} (i.e., a framework that automates scheduling).  The separation between mechanism and policy allows users to choose between using automatic scheduling or manual scheduling which provides more flexibility.

\vspace{-0.5cm}
\paragraph{Polyhedral compilers with a scheduling language.}
AlphaZ~\cite{yuki2012alphaz}, CHiLL~\cite{chill,Hall2010}, URUK~\cite{Girbal2006}, and Transformation Recipes~\cite{Hall:2009:LTR:2155247.2155251} are polyhedral frameworks and methods that were developed to allow users to express high-level transformations using scheduling commands. 
Since these frameworks are polyhedral, they can express any affine transformation.
The scheduling languages of these frameworks do not target distributed architectures though.  All of them also do not target GPUs (except the Transformation Recipes framework).  In comparison with these frameworks, \framework features scheduling commands for partitioning computations for execution on a distributed system, synchronization, distribution of data across nodes, mapping data to specific GPU memory hierarchy levels, and performing array packing.  The first four columns of Table~\ref{tab:related} show a summary of a comparison between \framework{} and three representative polyhedral frameworks (AlphaZ, PENCIL, and Pluto).

\begin{table}[t]
    \scriptsize
    \setlength\tabcolsep{1pt}
    \begin{tabular}{l|l|l|l|l|l}
        \hline
        
        \textbf{Feature} & \textbf{Tiramisu} & \textbf{AlphaZ} & \textbf{PENCIL} & \textbf{Pluto} & \textbf{Halide} \\\hline

        \textbf{CPU code generation} & \yes & \yes & \yes & \yes  & \yes \\\hline

        \textbf{GPU code generation} & \yes & \no & \yes & \yes  & \yes \\\hline

        \textbf{Distributed CPU code generation} & \yes & \no & \no & \yes  & \yes\\\hline
        
        \textbf{Distributed GPU code generation} & \yes & \no & \no & \no  & \no\\\hline

        \textbf{Support all affine loop transformations} & \yes & \yes & \yes & \yes  & \no\\\hline

        \textbf{Optimize data accesses} & \yes & \yes & \no & \no & \yes \\\hline

        \textbf{Commands for loop transformations} & \yes & \yes & \no & \no & \yes\\\hline

        \textbf{Commands for optimizing data accesses} & \yes & \yes & \no & \no & \yes \\\hline

        \textbf{Commands for performing data copies} & \yes & \no & \no & \no & \no \\\hline

        \textbf{Commands for memory hierarchies} & \yes & \no & \no & \no & \limited\\\hline

        \textbf{Expressing cyclic data-flow graphs} & \yes & \yes & \yes & \yes & \no \\\hline

        \textbf{Support non-rectangular iteration spaces} & \yes & \yes & \yes & \yes & \limited\\\hline

        \textbf{Instance-wise, exact dependence analysis} & \yes & \yes & \yes & \yes  & \no\\\hline
        
        \textbf{Compile-time affine-set emptiness check} & \yes & \yes & \yes & \yes  & \no\\\hline

        \textbf{Implement support for parametric tiling} & \no & \yes & \no & \no  & \yes\\\hline
    \end{tabular}
    \caption{Comparison Between Different Frameworks.}
    \label{tab:related}
    \vspace{-0.75cm}
\end{table}

\paragraph{Non-polyhedral compilers with a scheduling language.}
Halide~\cite{halide_12} is an image processing DSL that has a scheduling language; however, it uses intervals to represent iteration spaces instead of the polyhedral model.  This limits the expressiveness of Halide.
For example, Halide cannot naturally represent non-rectangular iteration spaces.  This makes certain Halide passes over-approximate non-rectangular iteration spaces which leads to less efficient code generation.  It also makes Halide over-approximate the amounts of data to communicate (send and receive) when generating distributed code.  It also prevents Halide from performing precise bounds inference for non-rectangular iteration spaces, as well as performing many complex affine transformations, such as iteration space skewing, which are all possible in \framework{}.

Halide does not have dependence analysis and thus it relies on conservative rules to determine whether a schedule is legal; for example, Halide does not allow the fusion of two loops (using the \texttt{compute\_with} command) if the second loop reads a value produced by the first loop.
While this rule avoids illegal fusion, it prevents fusing many legal common cases which may lead to suboptimal performance.
Halide also assumes the program has an acyclic dataflow graph in order to simplify checking the legality of scheduling commands. This prevents users from expressing many programs with cyclic dataflow.
It is possible in some cases to work around the above restrictions, but such methods are not general.
\framework{} avoids over-conservative constraints by relying on dependence analysis to check for the correctness of code transformations, enabling more possible schedules.  Table~\ref{tab:related} shows a comparison between \framework{} and Halide.

Other systems such as POET~\cite{Yi:2007ay} use an XML-based description of code and transformation behavior to parametrize loop transformations.  It uses a purely-syntactic transformation system to tune the loop transformations.  The use of syntactic transformations is less general than the polyhedral model used in \framework.

\paragraph{Non-polyhedral compilers with automatic scheduling.}

Delite \cite{chafi_domain-specific_2011} is a generic
framework for building DSL compilers using Lightweight Modular Staging (LMS) \cite{lms_staging_10}. It exposes several parallel computation patterns that DSLs can use to express parallelism.
NOVA~\cite{Collins:2014:NFL:2627373.2627375} and Lift~\cite{Steuwer:2017:LFD:3049832.3049841} are other IRs for DSL compilers.  They are functional languages that rely on a suite of higher-order functions such as map, reduce, and scan to express parallelism.
Weld~\cite{palkar2017weld} is another IR designed for the area of databases and data analytics and can be used to implement libraries, such as Numpy, and enable optimizations across different library calls.
\framework{} is complementary to all of these frameworks as \framework{} would allow them to perform and compose a large set of complex affine transformations that are easier to express in the polyhedral model.

%The Cyclops Tensor Framework (CTF)~\cite{solomonik2013cyclops} is a library for performing tensor contractions, primarily in the field of quantum chemistry. CTF automatically decomposes tensors using a communication-optimal tensor contraction algorithm and maps the computations to the underlying architecture. The framework targets distributed architectures, and can provide hybrid execution through the use of MPI and OpenMP.  Unlike \framework{} which is designed to be more general, CTF is designed mainly for tensor contractions.
%Chapel~\cite{Chamberlain:2007:PPC:1286120.1286123} is a parallel programming language that supports a Partitioned Global Address Space (PGAS) memory model~\cite{Krishnamurthy:1993:PPS:169627.169724}.  In this model, code can refer to variables and arrays regardless of whether they are stored in a local or remote memory.  Any necessary communication is automatically inserted by the compiler and executed at runtime.  Similarly, computations in Layer I of \framework{} refer to other computations in the same way regardless of whether they are stored or computed in local or remote memory (or in shared or global memory on GPU).  Specifying whether a computation is local or remote, how it is accessed and whether communication is needed are all done using scheduling commands at Layers II, III and IV. \framework{} provides a set of fine-grain commands that can implement any data-mapping and communication that a language like Chapel provides, yet \framework{} can complement Chapel by providing new capabilities such as advanced loop nest transformations and checking schedule validity.
% large has the advantage of enabling manual scheduling, data mapping and communication (while Chapel.