\section{Introduction}
\label{sec:intro}

High performance systems today range from multicore mobile phones to large scale heterogeneous supercomputers and cloud infrastructures equipped with GPUs and FPGAs.  Building a code optimization framework for these high performance systems requires solving many challenges.  In this paper we present an optimization framework that addresses four of these challenges.

The first is the \emph{multi-language} or the \emph{MPI+OpenMP +CUDA+HLS} challenge.
Most high performance computer systems are complex and increasingly heterogeneous; they may be single-node or distributed and may have GPUs~\cite{liao2014milkyway} and FPGAs~\cite{caulfield2017configurable}.
Achieving best performance requires taking full advantage of all these different architectures~\cite{yang2011hybrid}.
Writing code for such heterogeneous systems is difficult as each hardware architecture requires drastically different styles of code and optimization, all using different libraries and languages.  In addition, partitioning the program between heterogeneous components that correctly communicate and synchronize is difficult.  The current practice is to manually write and optimize the program in separate languages and libraries for each component. However, even a small change to the partitioning among heterogeneous units will often require a complete rewrite of the program.

The second challenge is that of \emph{memory dependence}.  Most intermediate representations use memory to communicate between program statements.  This creates memory-based dependences in the program and also means that the data-layout is chosen before deciding how the code should be scheduled (i.e., how it should be optimized and mapped to hardware).  Optimizing a program for different hardware architectures usually requires modifying the data-layout and eliminating memory-based dependences since they restrict optimization~\cite{maydan1992data}.  Thus, any data-layout specified before scheduling must be undone to allow more freedom for scheduling, and the code must be adapted to use the data-layout best-suited for the target hardware.
Applying these data-layout transformations and the elimination of memory-based dependences is challenging~\cite{gupta1997privatization,autoPrivatPeng,li_array_1992,feautrier_array_1988,midkiff_automatic_2012,maydan_array-data_1993,lefebvre_automatic_1998,Qui00,Darte_contraction_2005}.

The third challenge is the ability to \emph{optimize and generate efficient code}.  In many performance critical domains, users need code that achieves performance comparable to hand-optimized code.  
Generating such code requires combinations of non-trivial program transformations that optimization frameworks try to fully automate using cost models, heuristics~\cite{hall1995detecting}, and machine learning~\cite{tournavitis2009towards}.
While these automatic optimization frameworks provide productivity, they may not always achieve the desired level of performance.
Some frameworks also impose restrictions on the type of programs they support, since they cannot decide the correctness of schedules otherwise. While these language restrictions guarantee correctness, they may prevent users from applying valid combinations of optimizations. A more flexible way to guarantee the correctness of optimizations is needed.

The fourth challenge is that of \emph{representation}.  Lowering code to execute on complex hardware architectures requires numerous transformations that change program structure by introducing new loops, complex loop bounds, and non-trivial array accesses~\cite{wolf1991loop}.  Analyzing code generated by one of these transformations is challenging, which complicates composition with other transformations.  This problem can be mitigated by keeping loops within a single unified representation through all transformations.  However, many representations are inadequate or too conservative to support complex transformations or for tasks such as dependence analysis (necessary for deciding about the correctness of optimization) and tasks such as the computation of communication sets (data to send/receive in a distributed system).  
%Having a representation that is expressive enough is important for the success of an optimization framework targeting high performance systems.

This paper addresses these challenges by introducing \\\framework{}, a compiler optimization framework designed for targeting high performance systems.  \framework{} takes a high level representation of the program (pure algorithm and a set of commands specifying the schedule and data-layout), applies transformations on the representation and generates highly optimized code for the target architectures.  \framework{} is well suited for the implementation of data parallel algorithms (loop nests manipulating arrays).  It is designed to hide the complexity and large variety of execution platforms by providing a multi-layer representation suitable for transforming from high-level languages to multicore CPUs, GPUs, distributed machines, and FPGAs.

\framework{} addresses the first challenge by allowing users to partition their program and specify communication from the same source code using a simple set of scheduling commands. This simplifies programming distributed and heterogeneous systems: the algorithm does not change and only commands that control its execution and communication mapping require modification.
\framework{} also addresses the first challenge by using a novel multi-layer IR that fully separates the architecture-independent algorithm from the schedule, data-layout and communication.
%The top layer representation describes the pure algorithm using producer-consumer relationships without memory locations.
%The second layer specifies the order of the computations, along with which processor computes each value; this layer is suitable for performing a vast number of optimizations without dealing with concrete memory layouts.
%The third layer specifies where to store intermediate data before they are consumed.  The fourth layer adds any necessary communication and synchronization operations.
The multi-layer design makes the algorithm portable and makes it easier to perform each program transformation at the right layer of abstraction.
This multi-layer IR also helps \framework{} address the memory dependence challenge since this design separates data-layout from other transformations.

\framework{} addresses the challenge of optimization by separating mechanism from policy in scheduling and by removing heuristics and automatic decision-making.  This way, \framework{} allows full control over scheduling while still enabling integration with higher level frameworks for policy-making (deciding which optimization should be applied).  \framework{} guarantees correctness using dependence analysis and thus  does not need to impose undue restrictions on its input language to guarantee correctness.

The challenge of representation is addressed by using a unified framework based on polyhedral sets to represent the four layers.  This makes it simple for \framework{} to reason about and implement iteration space and data-layout transformations, since these are represented as transformations on polyhedral sets.  It also simplifies deciding about the legality of transformations based on dependence analysis.
The polyhedral framework also enables the application of a large set of complex optimizations. \framework{} does not extend the core of the polyhedral model, but rather it leverages the power of polyhedral compilation to target heterogeneous systems and to generate efficient code that matches kernels from highly optimized libraries such as the Intel MKL library. To the best of our knowledge, \framework{} is the first framework that uses polyhedral techniques and matches the performance of a single \emph{sgemm} kernel from Intel MKL.

In this paper we make the following contributions:

\begin{itemize}
  \item We introduce a unified framework that generates code for multiple high-performance architectures including multicore CPUs, GPUs, FPGAs, distributed machines, or any combination of these, using a set of simple scheduling commands to guide program transformations.

  %  \item We introduce a novel four-layer representation that separates the algorithm from code transformations and data-layout transformations, allowing for portability and simplifying the composition of architecture-specific lowering transformations.

  \item We demonstrate the first polyhedral framework that can generate code that matches a single \emph{sgemm} kernel from the highly optimized Intel MKL library.

  \item We demonstrate the power and viability of \framework{} by using it to write multiple linear algebra and neural network kernels and by using it as an optimization framework for the Halide~\cite{halide_12,DBLP:conf/pldi/Ragan-KelleyBAPDA13} image processing domain-specific language.

  \item We demonstrate the expressiveness of \framework{} by extending Halide with new capabilities such as expressing code with cyclic dataflow, performing precise bounds inference for non-rectangular iteration spaces, and performing advanced loop transformations such as skewing.

  \item We evaluate \framework{} and show that it matches or outperforms reference implementations on different hardware architecture backends (multicore CPUs, GPUs, FPGAs and distributed machines).
  %In particular, it matches the performance of kernels from the highly optimized libraries such as Intel MKL and show up to $4\times$ performance gains in Halide.
\end{itemize}